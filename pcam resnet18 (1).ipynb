{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e703b5c6-465e-4846-a1db-665d3a256f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272ae84e-ed2b-4b5d-a36f-959137b85c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is cuda\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Current device is', device)\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(2023)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5675e08-3e5c-41b6-b029-c4d94e4efae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KaggleHub cache deleted.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# 손상된 캐시 폴더 삭제\n",
    "cache_dir = os.path.expanduser(\"~/.cache/kagglehub\")\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "    print(\"KaggleHub cache deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b17b34-f56e-4ac2-98df-e3829e85468a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/andrewmvd/metastatic-tissue-classification-patchcamelyon?dataset_version_number=9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 7.47G/7.47G [11:35<00:00, 11.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\roobe\\.cache\\kagglehub\\datasets\\andrewmvd\\metastatic-tissue-classification-patchcamelyon\\versions\\9\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"andrewmvd/metastatic-tissue-classification-patchcamelyon\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec84a76a-71bb-4cc6-935d-608b6011966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 C:\\Users\\roobe/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon\n",
      "   └─ 9.complete\n",
      "📁 C:\\Users\\roobe/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon\\versions\n",
      "📁 C:\\Users\\roobe/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon\\versions\\9\n",
      "📁 C:\\Users\\roobe/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon\\versions\\9\\camelyonpatch_level_2_split_train_mask\n",
      "   └─ camelyonpatch_level_2_split_train_mask.h5\n",
      "📁 C:\\Users\\roobe/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon\\versions\\9\\Labels\n",
      "📁 C:\\Users\\roobe/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon\\versions\\9\\Labels\\Labels\n",
      "   └─ camelyonpatch_level_2_split_test_y.h5\n",
      "   └─ camelyonpatch_level_2_split_train_y.h5\n",
      "   └─ camelyonpatch_level_2_split_valid_y.h5\n",
      "📁 C:\\Users\\roobe/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon\\versions\\9\\Metadata\n",
      "📁 C:\\Users\\roobe/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon\\versions\\9\\Metadata\\Metadata\n",
      "   └─ test_metadata.csv\n",
      "   └─ train_metadata.csv\n",
      "   └─ valid_metadata.csv\n",
      "📁 C:\\Users\\roobe/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon\\versions\\9\\pcam\n",
      "   └─ test_split.h5\n",
      "   └─ training_split.h5\n",
      "   └─ validation_split.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 사용자 홈 기준으로 자동 경로 설정\n",
    "base_path = os.path.expanduser(\"~/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon\")\n",
    "\n",
    "# 모든 하위 버전 디렉토리 확인\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    print(\"📁\", root)\n",
    "    for f in files:\n",
    "        print(\"   └─\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f058fe8-c0e3-481c-b35f-ee276476f142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HDF5 파일 로드 성공!\n",
      "📂 포함된 키 목록: ['x']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "# Windows 사용자 홈 디렉토리 기준 경로\n",
    "data_dir = os.path.expanduser(\"~/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon/versions/9/pcam\")\n",
    "h5_file = os.path.join(data_dir, \"training_split.h5\")\n",
    "\n",
    "# HDF5 파일 내부 키 확인\n",
    "with h5py.File(h5_file, \"r\") as file:\n",
    "    print(\"✅ HDF5 파일 로드 성공!\")\n",
    "    print(\"📂 포함된 키 목록:\", list(file.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ddac761-4d9a-43aa-afd5-33dc8967686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f610ea62-fee3-422c-aaf1-77200a4e25b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 262144\n",
      "Validation dataset size: 32768\n",
      "Test dataset size: 32768\n"
     ]
    }
   ],
   "source": [
    "# HDF5 데이터셋 클래스 정의\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),#pcam 1채널인데 resnet 3채널만 받음-> rgb값 복사\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 3채널 이미지로 변경했기 때문에 정규화 값도 3개 필요\n",
    "])\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, image_h5_path, label_h5_path, transform=None):\n",
    "        self.image_h5_path = image_h5_path\n",
    "        self.label_h5_path = label_h5_path\n",
    "        self.transform = transform\n",
    "        # HDF5 파일 열기\n",
    "        with h5py.File(self.image_h5_path, \"r\") as img_file, h5py.File(self.label_h5_path, \"r\") as lbl_file:\n",
    "             self.images = img_file[\"x\"][:]  #이미지 데이터를 넘파이 배열로 변환해서 로드\n",
    "             self.labels = lbl_file[\"y\"][:]  #레이블 데이터를 넘파이 배열로 변환해서 로드\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      image = self.images[idx]\n",
    "      label = self.labels[idx]\n",
    "\n",
    "    # NumPy 배열을 PIL 이미지로 변환 (PIL은 HWC 순서를 기대함)\n",
    "      image = Image.fromarray(image.astype(np.uint8))  # (H, W, C)\n",
    "\n",
    "      if self.transform:\n",
    "         image = self.transform(image)  # transform이 (C, H, W) 등으로 바꿔줌\n",
    "      label = torch.tensor(label).squeeze().long()\n",
    "      return image, label\n",
    "\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "# 경로 수정\n",
    "data_dir = os.path.expanduser(\"~/.cache/kagglehub/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon/versions/9\")\n",
    "image_dir = os.path.join(data_dir, \"pcam\")  # ✅ 여기까지만\n",
    "label_dir = os.path.join(data_dir, \"Labels\", \"Labels\")  # ✅ 그대로 유지\n",
    "\n",
    "\n",
    "# 학습 데이터 로드\n",
    "train_dataset = HDF5Dataset(\n",
    "    image_h5_path=os.path.join(image_dir, \"training_split.h5\"),\n",
    "    label_h5_path=os.path.join(label_dir, \"camelyonpatch_level_2_split_train_y.h5\"),  # 수정된 레이블 경로\n",
    "    transform=transform\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 검증 데이터 로드\n",
    "val_dataset = HDF5Dataset(\n",
    "    image_h5_path=os.path.join(image_dir, \"validation_split.h5\"),\n",
    "    label_h5_path=os.path.join(label_dir, \"camelyonpatch_level_2_split_valid_y.h5\"),  # 수정된 레이블 경로\n",
    "    transform=transform\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)  #정확도가 낮게 나와서 배치 크기 키우기,학습률과 epoch도 올려서 정확도 높이려고 했어요\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_dataset = HDF5Dataset(\n",
    "    image_h5_path=os.path.join(image_dir, \"test_split.h5\"),\n",
    "    label_h5_path=os.path.join(label_dir, \"camelyonpatch_level_2_split_test_y.h5\"),  # 수정된 레이블 경로\n",
    "    transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 데이터 개수 확인\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1407ba39-aaaf-4491-be7f-37d73d8ec40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# 사전 학습된 ResNet-18 모델 불러오기\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model.eval()  # 추론 모드로 설정\n",
    "model.fc = nn.Linear(model.fc.in_features, 2) # PCam은 양성/음성 2클래스\n",
    "model = model.to(device)\n",
    "learning_rate = 0.01\n",
    "epochs = 4\n",
    "# define loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ed63743-04d6-41af-8866-5eca4668a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "# define loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39226bc0-8fcd-40b8-8d07-da5cd6959048",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = len(train_loader)\n",
    "val_batch = len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfbaae15-2b99-4f25-869d-1722ae464ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started.\n",
      "[Epoch  1] loss 0.2204  acc 91.19, val loss 0.4012  val acc 84.44\n",
      "[Epoch  2] loss 0.1697  acc 93.44, val loss 0.3879  val acc 86.11\n",
      "[Epoch  3] loss 0.1375  acc 94.83, val loss 0.3926  val acc 85.40\n",
      "[Epoch  4] loss 0.1146  acc 95.74, val loss 0.3244  val acc 86.79\n",
      "[Epoch  5] loss 0.0955  acc 96.50, val loss 0.5036  val acc 84.03\n",
      "[Epoch  6] loss 0.0781  acc 97.17, val loss 0.5284  val acc 83.44\n",
      "[Epoch  7] loss 0.0655  acc 97.63, val loss 0.5204  val acc 85.36\n",
      "[Epoch  8] loss 0.0547  acc 98.02, val loss 0.5985  val acc 84.37\n",
      "[Epoch  9] loss 0.0468  acc 98.35, val loss 0.6998  val acc 84.18\n",
      "[Epoch 10] loss 0.0401  acc 98.58, val loss 0.6593  val acc 84.37\n"
     ]
    }
   ],
   "source": [
    "print('Learning started.')\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "for epoch in range(epochs): # Use 'epochs' instead of 'epoch'\n",
    "    # training\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for X, Y in train_loader:\n",
    "        # image is already size of (1, 28, 28), no reshape\n",
    "        # label is not one-hot encoded\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = model.forward(X)\n",
    "        loss = criterion(Y_pred,Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss / train_batch\n",
    "\n",
    "        correct_prediction = torch.argmax(Y_pred, 1) == Y\n",
    "        correct += correct_prediction.sum()\n",
    "\n",
    "    train_acc = (100*correct/len(train_loader.dataset))\n",
    "    train_acc_list.append(train_acc.item())\n",
    "    train_loss_list.append(train_loss.item())\n",
    "\n",
    "\n",
    "    # validation                                                                                                                                                                                                                                    \n",
    "    with torch.no_grad(): #validation 에서는 gardient 업데이트x\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for X, Y in val_loader:\n",
    "            X_val = X.to(device)\n",
    "            Y_val = Y.to(device).squeeze()\n",
    "\n",
    "            Y_pred = model.forward(X_val)\n",
    "            loss = criterion(Y_pred, Y_val)\n",
    "\n",
    "            val_loss += loss / val_batch\n",
    "\n",
    "            correct_prediction = torch.argmax(Y_pred, 1) == Y_val\n",
    "            correct += correct_prediction.sum()\n",
    "\n",
    "        val_acc = (100*correct/len(val_loader.dataset))\n",
    "        val_acc_list.append(val_acc.item())\n",
    "        val_loss_list.append(val_loss.item())\n",
    "\n",
    "    print(\"[Epoch %2d] loss %.4f  acc %.2f, val loss %.4f  val acc %.2f\"\n",
    "          % (epoch+1, train_loss, train_acc, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b91ac8f-7263-4771-bd8a-5b335abcadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list, range(len(val_loss_list)), val_loss_list)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.title('Loss')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(len(train_acc_list)), train_acc_list, range(len(val_acc_list)),val_acc_list)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c11a2-bd17-4530-8abb-577f73baf601",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    # Test the model using test sets\n",
    "    for X, Y in test_loader:\n",
    "        X_test = X.to(device)\n",
    "        Y_test = Y.to(device)\n",
    "\n",
    "        Y_pred = model.forward(X_test)\n",
    "        prediction = torch.argmax(Y_pred, 1)\n",
    "        correct_prediction = prediction == Y_test\n",
    "        correct += correct_prediction.sum()\n",
    "\n",
    "    accuracy = 100*correct/len(test_loader.dataset)  \n",
    "    print('Test set Accuracy: %.2f' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbff2083-9d32-48db-acdf-4a6a51078fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1], dtype=uint8), array([147471, 147441], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = np.concatenate([train_dataset.labels, val_dataset.labels])\n",
    "print(np.unique(labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b223304-be82-4d11-a06f-58879c30c2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight: requires_grad = True\n",
      "bn1.weight: requires_grad = True\n",
      "bn1.bias: requires_grad = True\n",
      "layer1.0.conv1.weight: requires_grad = True\n",
      "layer1.0.bn1.weight: requires_grad = True\n",
      "layer1.0.bn1.bias: requires_grad = True\n",
      "layer1.0.conv2.weight: requires_grad = True\n",
      "layer1.0.bn2.weight: requires_grad = True\n",
      "layer1.0.bn2.bias: requires_grad = True\n",
      "layer1.1.conv1.weight: requires_grad = True\n",
      "layer1.1.bn1.weight: requires_grad = True\n",
      "layer1.1.bn1.bias: requires_grad = True\n",
      "layer1.1.conv2.weight: requires_grad = True\n",
      "layer1.1.bn2.weight: requires_grad = True\n",
      "layer1.1.bn2.bias: requires_grad = True\n",
      "layer2.0.conv1.weight: requires_grad = True\n",
      "layer2.0.bn1.weight: requires_grad = True\n",
      "layer2.0.bn1.bias: requires_grad = True\n",
      "layer2.0.conv2.weight: requires_grad = True\n",
      "layer2.0.bn2.weight: requires_grad = True\n",
      "layer2.0.bn2.bias: requires_grad = True\n",
      "layer2.0.downsample.0.weight: requires_grad = True\n",
      "layer2.0.downsample.1.weight: requires_grad = True\n",
      "layer2.0.downsample.1.bias: requires_grad = True\n",
      "layer2.1.conv1.weight: requires_grad = True\n",
      "layer2.1.bn1.weight: requires_grad = True\n",
      "layer2.1.bn1.bias: requires_grad = True\n",
      "layer2.1.conv2.weight: requires_grad = True\n",
      "layer2.1.bn2.weight: requires_grad = True\n",
      "layer2.1.bn2.bias: requires_grad = True\n",
      "layer3.0.conv1.weight: requires_grad = True\n",
      "layer3.0.bn1.weight: requires_grad = True\n",
      "layer3.0.bn1.bias: requires_grad = True\n",
      "layer3.0.conv2.weight: requires_grad = True\n",
      "layer3.0.bn2.weight: requires_grad = True\n",
      "layer3.0.bn2.bias: requires_grad = True\n",
      "layer3.0.downsample.0.weight: requires_grad = True\n",
      "layer3.0.downsample.1.weight: requires_grad = True\n",
      "layer3.0.downsample.1.bias: requires_grad = True\n",
      "layer3.1.conv1.weight: requires_grad = True\n",
      "layer3.1.bn1.weight: requires_grad = True\n",
      "layer3.1.bn1.bias: requires_grad = True\n",
      "layer3.1.conv2.weight: requires_grad = True\n",
      "layer3.1.bn2.weight: requires_grad = True\n",
      "layer3.1.bn2.bias: requires_grad = True\n",
      "layer4.0.conv1.weight: requires_grad = True\n",
      "layer4.0.bn1.weight: requires_grad = True\n",
      "layer4.0.bn1.bias: requires_grad = True\n",
      "layer4.0.conv2.weight: requires_grad = True\n",
      "layer4.0.bn2.weight: requires_grad = True\n",
      "layer4.0.bn2.bias: requires_grad = True\n",
      "layer4.0.downsample.0.weight: requires_grad = True\n",
      "layer4.0.downsample.1.weight: requires_grad = True\n",
      "layer4.0.downsample.1.bias: requires_grad = True\n",
      "layer4.1.conv1.weight: requires_grad = True\n",
      "layer4.1.bn1.weight: requires_grad = True\n",
      "layer4.1.bn1.bias: requires_grad = True\n",
      "layer4.1.conv2.weight: requires_grad = True\n",
      "layer4.1.bn2.weight: requires_grad = True\n",
      "layer4.1.bn2.bias: requires_grad = True\n",
      "fc.weight: requires_grad = True\n",
      "fc.bias: requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad = {param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a656389-2c90-4a3a-b9e0-1252c171614e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
